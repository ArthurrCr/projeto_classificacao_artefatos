{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Para rodar no notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Adiciona o diretório src ao sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "# Importar o TensorFlow Addons\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Ignorar avisos desnecessários\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Importar as funções de data augmentation\n",
    "from funcoes_augmentation import load_and_preprocess_image, random_rotation, adjust_brightness, flip_horizontal, random_zoom, random_shear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo de Classificação de Artefatos Arqueológicos\n",
    "\n",
    "Neste notebook, realizaremos o treinamento de um modelo de classificação de imagens de artefatos arqueológicos utilizando uma rede neural convolucional baseada na EfficientNet. O objetivo é avaliar o impacto de diferentes técnicas de data augmentation no desempenho do modelo, testando cada técnica individualmente e buscando os níveis ótimos antes de combiná-las.\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivos:**\n",
    "\n",
    "- Treinar um modelo baseline sem data augmentation.\n",
    "- Aplicar técnicas de data augmentation individualmente:\n",
    "  - Rotação\n",
    "  - Alteração de Brilho\n",
    "  - Espelhamento Horizontal\n",
    "  - Zoom In e Zoom Out\n",
    "  - Cisalhamento (Shear)\n",
    "- Comparar o desempenho do modelo com cada técnica.\n",
    "- Identificar as melhores técnicas e combiná-las.\n",
    "- Documentar os resultados para análise posterior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretórios de dados\n",
    "train_dir = '../dados/brutos/train'\n",
    "test_dir = '../dados/brutos/test'\n",
    "\n",
    "# Lista das classes\n",
    "classes = ['classe1_Vidro', 'classe2_Ceramica', 'classe3_Litico', 'classe4_Louca']\n",
    "\n",
    "# Mapeamento das classes para índices\n",
    "class_indices = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "# Parâmetros do modelo\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "num_classes = len(classes)\n",
    "epochs = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels(data_dir, classes):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(data_dir, cls)\n",
    "        cls_index = class_indices[cls]\n",
    "        for img_name in os.listdir(cls_dir):\n",
    "            img_path = os.path.join(cls_dir, img_name)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(cls_index)\n",
    "    return image_paths, labels\n",
    "\n",
    "# Dados de treinamento\n",
    "train_image_paths, train_labels = get_image_paths_and_labels(train_dir, classes)\n",
    "\n",
    "# Dados de validação (teste)\n",
    "test_image_paths, test_labels = get_image_paths_and_labels(test_dir, classes)\n",
    "\n",
    "# Converter para tensores\n",
    "train_image_paths = tf.constant(train_image_paths)\n",
    "train_labels = tf.constant(train_labels)\n",
    "\n",
    "test_image_paths = tf.constant(test_image_paths)\n",
    "test_labels = tf.constant(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos o dataset de teste (validação) sem data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de validação\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_image_paths, test_labels))\n",
    "\n",
    "test_ds = test_ds.map(\n",
    "    lambda x, y: (load_and_preprocess_image(x), y)\n",
    ").batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar o Modelo Base EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_model(num_classes):\n",
    "    # Carregar o modelo base\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "    base_model.trainable = False  # Transfer learning - congelar as camadas convolucionais\n",
    "\n",
    "    # Construir a arquitetura do modelo\n",
    "    inputs = layers.Input(shape=(img_height, img_width, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos o otimizador, a função de perda e as métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16705208/16705208 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo\n",
    "model = create_model(num_classes)\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar o modelo em diferentes cenários:\n",
    "\n",
    "- Baseline: Sem data augmentation.\n",
    "- Com cada técnica de data augmentation individualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar o Dataset de Treinamento Sem Data Augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de treinamento sem data augmentation\n",
    "train_ds_baseline = tf.data.Dataset.from_tensor_slices((train_image_paths, train_labels))\n",
    "\n",
    "train_ds_baseline = train_ds_baseline.map(\n",
    "    lambda x, y: (load_and_preprocess_image(x), y)\n",
    ").shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 3/48 [>.............................] - ETA: 1:10 - loss: 1.2751 - accuracy: 0.3542"
     ]
    }
   ],
   "source": [
    "# Recriar o modelo para garantir que começamos do zero\n",
    "model = create_model(num_classes)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "history_baseline = model.fit(\n",
    "    train_ds_baseline,\n",
    "    validation_data=test_ds,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Salvar o histórico\n",
    "histories = {}\n",
    "histories['baseline'] = history_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
